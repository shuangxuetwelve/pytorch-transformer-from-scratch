{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer的PyTorch从头实现"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 依赖和常量"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 依赖"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此项目使用了PyTorch，请参考以下页面安装PyTorch：\n",
    "[https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)\n",
    "\n",
    "除了PyTorch外，此项目还需要以下Python库：\n",
    "* torchtext\n",
    "\n",
    "可以使用以下pip命令安装它们：\n",
    "```\n",
    "pip install torchtext\n",
    "```\n",
    "\n",
    "此项目使用了spaCy tokenizer，需要使用以下命令安装spaCy语言包de_core_news_sm和en_core_web_sm：\n",
    "```\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download de_core_news_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python imports.\n",
    "from enum import Enum\n",
    "import os\n",
    "\n",
    "# Third-party imports.\n",
    "import torch\n",
    "import torchtext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset types.\n",
    "class Dataset(Enum):\n",
    "    MULTI30K = 1\n",
    "\n",
    "# Define which dataset to use.\n",
    "DATASET = Dataset.MULTI30K"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据准备"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用[IWSLT 2017](https://wit3.fbk.eu/2017-01)中的英语翻德语数据集。torchtext.datasets集成了这个数据集，参考[文档](https://pytorch.org/text/stable/datasets.html#iwslt2017)。\n",
    "\n",
    "其中，train、dev和test数据集各自的规模如下：\n",
    "* train：206112\n",
    "* dev: 888\n",
    "* test: 1568"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 辅助函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算数据集中数据的数量。\n",
    "def count(dataset):\n",
    "    count = 0\n",
    "    for item in dataset:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "# 显示数据集的前五条数据。\n",
    "def display_samples(dataset, num):\n",
    "    iterator = iter(dataset)\n",
    "    try:\n",
    "        for i in range(num):\n",
    "            print(f'{i+1}: {next(iterator)}')\n",
    "    except StopIteration:\n",
    "        print(f'[Error] Size of dataset is smaller than {num}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 数据加载"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载train、dev和test数据集。这些数据集的数据类型为torch.utils.data.datapipes.iter.grouping.ShardingFilterIterDataPipe。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in train dataset: 206112\n",
      "Number of items in dev dataset: 888\n",
      "Number of items in test dataset: 1568\n",
      "Samples from train dataset:\n",
      "1: ('Thank you so much, Chris.\\n', 'Vielen Dank, Chris.\\n')\n",
      "2: (\"And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\\n\", 'Es ist mir wirklich eine Ehre, zweimal auf dieser Bühne stehen zu dürfen. Tausend Dank dafür.\\n')\n",
      "3: ('I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.\\n', 'Ich bin wirklich begeistert von dieser Konferenz, und ich danke Ihnen allen für die vielen netten Kommentare zu meiner Rede vorgestern Abend.\\n')\n",
      "4: ('And I say that sincerely, partly because  I need that.\\n', 'Das meine ich ernst, teilweise deshalb -- weil ich es wirklich brauchen kann!\\n')\n",
      "5: ('Put yourselves in my position.\\n', 'Versetzen Sie sich mal in meine Lage!\\n')\n"
     ]
    }
   ],
   "source": [
    "# 加载train、dev和test数据集。\n",
    "train, dev, test = torchtext.datasets.IWSLT2017(language_pair=('en', 'de'))\n",
    "\n",
    "# 打印train、dev和test数据集。\n",
    "print(f'Number of items in train dataset: {count(train)}')\n",
    "print(f'Number of items in dev dataset: {count(dev)}')\n",
    "print(f'Number of items in test dataset: {count(test)}')\n",
    "\n",
    "# 打印train数据集前五条数据。\n",
    "print('Samples from train dataset:')\n",
    "display_samples(train, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 数据tokenization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用[spaCy](https://spacy.io/)库来进行tokenizer。spaCy库有多种语言的tokenization，每种语言包含多个pipeline。一种语言的不同pipeline所适用的场景不同，比如中文的pipeline zh_core_web_sm是使用web数据训练的小型通用pipeline，而zh_core_web_lg则是使用web数据训练的大型通用pipeline。可以在这篇文档中找到spaCy pipeline的命名规则、所支持的语言以及每种语言包含的pipeline。\n",
    "\n",
    "torchtext提供了一个方便的创建spaCy tokenizer的函数：torchtext.data.get_tokenizer。它支持包含spaCy在内的多种tokenization库：basic_english、revtok、subword、spacy、moses。\n",
    "\n",
    "在以下代码中，我们使用pipeline en_core_web_sm来对英语句子进行tokenization，使用pipeline de_core_news_sm来对德语句子进行tokenization。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you so much, Chris.\n",
      " -> ['Thank', 'you', 'so', 'much', ',', 'Chris', '.', '\\n']\n",
      "Vielen Dank, Chris.\n",
      " -> ['Vielen', 'Dank', ',', 'Chris', '.', '\\n']\n"
     ]
    }
   ],
   "source": [
    "# 加载spaCy中英语和德语的tokenizer。\n",
    "tokenizer_en = torchtext.data.get_tokenizer('spacy', language='en_core_web_sm')\n",
    "tokenizer_de = torchtext.data.get_tokenizer('spacy', language='de_core_news_sm')\n",
    "\n",
    "# 打印train数据集中第一条数据的tokenization结果。\n",
    "iterator_train = iter(train)\n",
    "sentence = next(iterator_train)\n",
    "sentence_en = sentence[0]\n",
    "sentence_de = sentence[1]\n",
    "print(f'{sentence_en} -> {tokenizer_en(sentence_en)}')\n",
    "print(f'{sentence_de} -> {tokenizer_de(sentence_de)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 创建vocabulary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在创建vacabulary时，我们考虑四种特殊token：\n",
    "* `<s>`: 句首token，表示一个句子的开始。\n",
    "* `</s>`：句尾token，表示一个句子的结束。\n",
    "* `<blank>`：空白token，用于padding。\n",
    "* `<unk>`：未知token，表示不在vocabulary中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vocabularies in the source language: 37726\n",
      "Number of vocabularies in the target language: 62261\n"
     ]
    }
   ],
   "source": [
    "def yield_tokens(dataset, tokenizer, index):\n",
    "    for item in dataset:\n",
    "        yield tokenizer(item[index])\n",
    "\n",
    "def create_vocabs(dataset, tokenizer_source, tokenizer_target):\n",
    "    vocab_source = torchtext.vocab.build_vocab_from_iterator(\n",
    "        yield_tokens(dataset, tokenizer_source, 0),\n",
    "        specials=['<s>', '</s>', '<blank>', '<unk>'],\n",
    "        min_freq=2,\n",
    "    )\n",
    "    vocab_target = torchtext.vocab.build_vocab_from_iterator(\n",
    "        yield_tokens(dataset, tokenizer_target, 1),\n",
    "        specials=['<s>', '</s>', '<blank>', '<unk>'],\n",
    "        min_freq=2,\n",
    "    )\n",
    "\n",
    "    vocab_source.set_default_index(vocab_source['<unk>'])\n",
    "    vocab_target.set_default_index(vocab_source['<unk>'])\n",
    "\n",
    "    return vocab_source, vocab_target\n",
    "\n",
    "# 优先从temp/vocabs.pt中读取vocabulary，如果文件不存在，那么重新计算并存储到文件中。\n",
    "# 从文件读取vocabulary比重新计算快得多，在我的计算机上，重新计算花费24秒，重新读取花费0.1秒。\n",
    "def load_or_create_vocabs():\n",
    "    if not os.path.exists('temp/vocabs.pt'):\n",
    "        vocab_source, vocab_target = create_vocabs(train+dev+test, tokenizer_en, tokenizer_de)\n",
    "        if not os.path.exists('temp'):\n",
    "            os.makedirs('temp')\n",
    "        torch.save((vocab_source, vocab_target), 'temp/vocabs.pt')\n",
    "    else:\n",
    "        vocab_source, vocab_target = torch.load('temp/vocabs.pt')\n",
    "    return vocab_source, vocab_target\n",
    "\n",
    "vocab_source, vocab_target = load_or_create_vocabs()\n",
    "\n",
    "# 打印输入数据集与输出数据集的vocabulary规模。\n",
    "print(f'Number of vocabularies in the source language: {len(vocab_source)}')\n",
    "print(f'Number of vocabularies in the target language: {len(vocab_target)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 句子转换为index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(\n",
    "    batch,\n",
    "    tokenizer_source,\n",
    "    tokenizer_target,\n",
    "    vocab_source,\n",
    "    vocab_target,\n",
    "    padding_max = 128,\n",
    "    padding_value = 2,\n",
    "):\n",
    "    # 在上面的实现中，source和target的<s>与</s>所对应的index应该是一致的，但为了逻辑的自洽性，我们在这里不做此假设。\n",
    "    bos_source = torch.tensor([vocab_source['<s>']]) # Beginning of source sentence.\n",
    "    eos_source = torch.tensor([vocab_source['</s>']]) # End of source sentence.\n",
    "    bos_target = torch.tensor([vocab_target['<s>']]) # Beginning of target sentence.\n",
    "    eos_target = torch.tensor([vocab_target['</s>']]) # End of target sentence.\n",
    "\n",
    "    # source_list中的每一项为经过padding操作的原句子的index tensor。\n",
    "    source_list = []\n",
    "    # target_list中的每一项为经过padding操作的目标句子的index tensor。\n",
    "    target_list = []\n",
    "\n",
    "    for sentence_source, sentence_target in batch:\n",
    "        # 将句子转换为index tensor。\n",
    "        indices_source = torch.cat([bos_source, vocab_source(tokenizer_source(sentence_source)), eos_source])\n",
    "        indices_target = torch.cat([bos_target, vocab_target(tokenizer_target(sentence_target)), eos_target])\n",
    "\n",
    "        # 将句子的index tensor进行padding操作，从而使所有句子的长度一致。\n",
    "        source_list.append(\n",
    "            torch.nn.functional.pad(\n",
    "                indices_source,\n",
    "                pad=(0, padding_max-len(indices_source)),\n",
    "                value=padding_value,\n",
    "            )\n",
    "        )\n",
    "        target_list.append(\n",
    "            torch.nn.functional.pad(\n",
    "                indices_target,\n",
    "                pad=(0, padding_max-len(indices_target)),\n",
    "                value=padding_value,\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # 将source_list转换为一个大的tensor，其维度为(len(batch), padding_max)。\n",
    "    source = torch.stack(source_list)\n",
    "    # 将target_list转换为一个大的tensor，其维度为(len(batch), padding_max)。\n",
    "    target = torch.stack(target_list)\n",
    "\n",
    "    return source, target"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28d349a2b51ce633eb90298c209eaf4844980ca37d4c8ff0f162257187ae6212"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
